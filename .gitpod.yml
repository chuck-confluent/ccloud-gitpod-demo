github:
  prebuilds:
    # enable for the default branch (defaults to true)
    master: true
    # enable for all branches in this repo (defaults to false)
    branches: true
    # enable for pull requests coming from this repo (defaults to true)
    pullRequests: true
    # add a "Review in Gitpod" button as a comment to pull requests (defaults to false)
    addComment: true
    # configure whether Gitpod registers itself as a status check to pull requests
    addCheck: false

image:
  file: .gitpod.Dockerfile

tasks:

  - name: producer
    command: |
      gp sync-await properties
      mvn exec:java -Dexec.mainClass="io.confluent.examples.clients.cloud.ProducerAvroExample" -Dexec.args="./ccloud.properties test2"
      gp sync-done producer
      sleep 5
      mvn exec:java -Dexec.mainClass="io.confluent.examples.clients.cloud.ProducerAvroExample" -Dexec.args="./ccloud.properties test2"

  - name: consumer
    command: |
      gp sync-await properties
      gp sync-await producer
      mvn exec:java -Dexec.mainClass="io.confluent.examples.clients.cloud.ConsumerAvroExample" -Dexec.args="./ccloud.properties test2"

  - name: kafka streams
    command: |
      gp sync-await properties
      gp sync-await producer
      mvn exec:java -Dexec.mainClass="io.confluent.examples.clients.cloud.StreamsAvroExample" -Dexec.args="./ccloud.properties test2"


  # Dynamically create unique ccloud properties file
  # Install Java project dependencies at build time
  # Then ccloud login at launch time
  - name: setup
    before: |
      cat<<-EOF>ccloud.properties && gp sync-done properties
      # Required connection configs for Kafka producer, consumer, and admin
      bootstrap.servers=$CCLOUD_BOOTSTRAP_SERVER
      security.protocol=SASL_SSL
      sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule   required username='$CCLOUD_API_KEY'   password='$CCLOUD_API_SECRET';
      sasl.mechanism=PLAIN
      # Required for correctness in Apache Kafka clients prior to 2.6
      client.dns.lookup=use_all_dns_ips

      # Best practice for Kafka producer to prevent data loss
      acks=all

      # Required connection configs for Confluent Cloud Schema Registry
      schema.registry.url=$CCLOUD_SR_URL
      basic.auth.credentials.source=USER_INFO
      basic.auth.user.info=$CCLOUD_SR_API_KEY:$CCLOUD_SR_API_SECRET
      EOF
    init: mvn clean package
    command: confluent login --save && confluent environment use $CCLOUD_ENV_ID

vscode:
  extensions:
    - vscjava.vscode-java-pack